# eBPF for GPU

## Cross layer Observability of GPU Workloads with eBPF

Widely used for ML workloads, GPUs are typically SIMT accelerators with threads in warps on SMs, organized into blocks, launched as kernels, using multi-level memory hierarchies (registers, shared/LDS, L2, device memory) and limited preemption. This complexity creates rich but challenging behavior patterns for observability and customization. Today, many tracing tools for GPU workloads sit at the CPU boundary (e.g. probes on CUDA userspace libraries or kernel drivers), which gives you host-side events, but treats the device as a black box: little visibility inside a running kernel, weak linkage to stalls or memory traffic, and no safe way to adapt behavior in-flight. GPU specific profilers(e.g. CUPTI, GTPin, Nvbit, Neutrino) provide device-side visibility, but they are often siloed from eBPF pipelines, make it harder to corelate with events on CPUs.

We prototype offloading eBPF into GPU device contexts by defining GPU-side attach points (CUDA device function entry/exit, thread begin/end, barrier/sync, memory ops, etc) and compiling eBPF programs into device bytecode (PTX/SPIR-V), with verifier, helper, and map support for on-device execution. Built on top of bpftime, this approach can be 3-10x faster than NVBit, is not vendor-locked, and works with Linux kernel eBPF programs like kprobes and uprobes. This enables GPU extensions like fine-grained profiling at the GPU thread, warp or instruction level, adaptive GPU kernel optimization, and programmable scheduling across SMs with eBPF. It can also help accelerate some existing eBPF applications.

The goal of this talk is to explore the usecases, challenges and lessons learned from extending eBPF's programming model to GPUs.

https://github.com/eunomia-bpf/bpftime/tree/master/example/gpu


## Network

### Programmable GPU Networking: Why We Need Both Bypass and Programmability

In production today, GPU networking has shifted from CPU-centric to CPU-bypass architectures, where GPU+hardware can request/response directly from the GPU side without CPU involvement. Using GPUDirect RDMA for direct NIC↔GPU DMA, GPU-initiated communication (e.g., NVSHMEM/IBGDA) where kernels can post work without CPU round-trips, and in-network aggregation like SHARP, these systems eliminate memory copies and synchronization overhead, reduce host CPU load, and improve strong-scaling efficiency across nodes. This CPU-bypass approach is now the default for large-scale distributed training and is spreading into low-latency inference and inline datapaths, including prefill-decode disaggregated inference where prefill and decode stages run on separate machines.

However, while fixed libraries (NCCL/collectives) and fixed hardware algorithms cover common cases and deliver fast performance for major workloads, they prove inadequate for emerging needs. Industry experience reveals critical limitations: Meta reported that DCQCN—a popular RDMA NIC congestion control algorithm—fails for LLM training workloads with low flow entropy and high traffic burstiness, forcing them to disable NIC-level congestion control and implement traffic scheduling at the application layer instead. Similarly, DeepSeek disabled congestion control when running large-scale all-to-all operations for serving MoE models. Yet running large-scale RDMA networks without congestion control is brittle, leading to deadlocks, head-of-line blocking, and pervasive congestion. Alibaba observed severe performance degradation in collective communication during LLM training due to high flow collisions caused by RDMA NICs supporting only single-flow/path per connection, forcing them to redesign their entire network topology with a rail-optimized dual-plane architecture.

These challenges stem from ML clusters' operational reality. Training and high-QPS inference workloads are long-running, multi-tenant, and tightly pipelined across CPU schedulers, NIC queues, and GPUs (compute + HBM). Jobs pin gigabytes of model state, build CUDA graphs, warm caches, and join collectives—restarting to change even simple policies (sampling, rate-limiting, tagging, drop/aggregate logic) burns minutes of downtime and can deschedule expensive nodes. The hot path must maintain nanosecond-class latency: per-token latency in LLM serving and per-step time in distributed training are dominated by GPU kernels, NVLink/PCIe transfers, GPUDirect RDMA, and collectives. Any CPU detour or heavyweight hook widens tail latency or elongates iteration time at scale. Meanwhile, optimal decisions require cross-layer coordination—the right action on one device depends on state from others (e.g., throttle NIC sends when GPU RX rings back up, downsample traces when CPU backlog spikes, bias all-reduce shard placement to fabric topology). Safe rollout/rollback is essential because policies are experimental: a bad counter or pointer can wedge a kernel, trip the driver watchdog, or poison multi-node training; production demands guardrails to load, canary, and instantly revert without killing jobs.

Operators increasingly need programmable hooks close to the GPU/NIC fast path for workload-specific logic: packet shaping and congestion hints, inline compression/encryption, model-aware routing, per-tenant QoS, and fine-grained telemetry. The challenge is that current systems force a false choice between programmability and bypass—we either get CPU-bypass performance with fixed, inflexible hardware algorithms, or we get programmability but sacrifice the performance benefits of bypass by introducing CPU round-trips. What we actually need is both: a uniform, runtime extension plane next to the data path with a shared control plane that can update behavior in place, keep the fast path on-device, coordinate CPU/NIC/GPU decisions, and roll back safely—all without touching or restarting application kernels.

eBPF fits this character perfectly. It provides verified, bounded, hot-attach programs with shared maps for cross-device state, enabling runtime policy updates while meeting the nanosecond-latency requirements of large-scale training and inference. Similar to how hXDP brings programmable packet processing to FPGA NICs while using minimal resources, extending eBPF to GPU networking contexts would enable adaptive, workload-aware network behavior without sacrificing the performance benefits of CPU-bypass architectures. This approach addresses the operational reality of modern ML infrastructure: the need for safe, fast, programmable control over increasingly complex distributed systems where fixed solutions inevitably lag behind evolving workload demands.

**References:**
- [NVIDIA Dynamo: A Low-Latency Distributed Inference Framework](https://developer.nvidia.com/blog/introducing-nvidia-dynamo-a-low-latency-distributed-inference-framework-for-scaling-reasoning-ai-models/)
- [Disaggregated Inference - Modular](https://docs.modular.com/mammoth/disaggregated-inference/)
- Zhu et al., "Congestion Control for Large-Scale RDMA Deployments" (DCQCN), SIGCOMM 2015
- Gangidi et al., "Network Challenges in LLM Training", Meta, 2024
- Qian et al., "Network Optimization for LLM Training", Alibaba, 2024
- DeepSeek, "Scaling MoE Models", 2025

